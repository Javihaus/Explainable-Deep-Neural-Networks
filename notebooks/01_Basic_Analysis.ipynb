{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Neural Network Topological Analysis\n",
    "\n",
    "This notebook demonstrates the basic usage of the neural topology framework for analyzing the topological structure of neural network layers.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We will:\n",
    "1. Load the COVID-19 drug discovery dataset\n",
    "2. Build and train a neural network\n",
    "3. Extract layer activations\n",
    "4. Perform topological analysis using:\n",
    "   - Persistent homology (Vietoris-Rips)\n",
    "   - Mapper algorithm\n",
    "   - UMAP projections\n",
    "5. Visualize the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install -e ..[tda,tensorflow,visualization]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import neural topology framework\n",
    "from neural_topology import (\n",
    "    TopologicalAnalyzer,\n",
    "    NeuralExtractor,\n",
    "    DataProcessor\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Neural Topology Framework - Basic Analysis\")\n",
    "print(\"==========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COVID-19 drug discovery dataset\n",
    "data_path = '../DDH_Data_with_Properties.csv'\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(data_path)\n",
    "    print(f\"Dataset loaded successfully: {data.shape}\")\n",
    "    print(f\"Columns: {list(data.columns)[:5]}...\")  # Show first 5 columns\n",
    "except FileNotFoundError:\n",
    "    print(f\"Dataset not found at {data_path}\")\n",
    "    print(\"Please ensure the dataset is in the correct location\")\n",
    "    # Create synthetic data for demonstration\n",
    "    np.random.seed(42)\n",
    "    data = pd.DataFrame({\n",
    "        **{f'feature_{i}': np.random.randn(100) for i in range(33)},\n",
    "        'pIC50': np.random.uniform(-1, 3, 100)\n",
    "    })\n",
    "    print(\"Using synthetic dataset for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Clean data\n",
    "if 'pIC50' in data.columns:\n",
    "    clean_data = data[data['pIC50'] != 'BLINDED'] if 'BLINDED' in data['pIC50'].values else data\n",
    "    clean_data = clean_data.dropna()\n",
    "    \n",
    "    # Convert pIC50 to numeric if needed\n",
    "    clean_data['pIC50'] = pd.to_numeric(clean_data['pIC50'], errors='coerce')\n",
    "    clean_data = clean_data.dropna()\n",
    "    \n",
    "    print(f\"Clean dataset: {clean_data.shape}\")\n",
    "else:\n",
    "    clean_data = data\n",
    "    print(\"Using full dataset (no pIC50 column found)\")\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = [col for col in clean_data.select_dtypes(include=[np.number]).columns if col != 'pIC50']\n",
    "X = clean_data[feature_cols].values\n",
    "\n",
    "if 'pIC50' in clean_data.columns:\n",
    "    y = clean_data['pIC50'].values\n",
    "    y_binary = (y > np.median(y)).astype(int)  # Binary classification\n",
    "else:\n",
    "    y_binary = np.random.binomial(1, 0.5, len(X))  # Random binary for demo\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_binary, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Creation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build neural network model\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    from tensorflow.keras import regularizers\n",
    "    \n",
    "    # Build model\n",
    "    model = Sequential([\n",
    "        Dense(50, activation='relu', \n",
    "              kernel_regularizer=regularizers.l2(0.05),\n",
    "              input_dim=X_train.shape[1], name='dense_1'),\n",
    "        Dense(40, activation='relu',\n",
    "              kernel_regularizer=regularizers.l2(0.05), name='dense_2'),\n",
    "        Dense(20, activation='relu',\n",
    "              kernel_regularizer=regularizers.l2(0.05), name='dense_3'),\n",
    "        Dense(1, activation='sigmoid',\n",
    "              kernel_regularizer=regularizers.l2(0.05), name='output')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"Model architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    tensorflow_available = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"TensorFlow not available. Please install with: pip install tensorflow\")\n",
    "    tensorflow_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "if tensorflow_available:\n",
    "    print(\"Training neural network...\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=10,\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    print(f\"Training accuracy: {train_acc:.3f}\")\n",
    "    print(f\"Test accuracy: {test_acc:.3f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping model training - TensorFlow not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Activation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tensorflow_available:\n",
    "    # Initialize neural extractor\n",
    "    extractor = NeuralExtractor()\n",
    "    \n",
    "    # Extract activations from all layers\n",
    "    print(\"Extracting layer activations...\")\n",
    "    activations = extractor.extract_all_layers(model, X_test)\n",
    "    \n",
    "    print(f\"Extracted activations from {len(activations)} layers:\")\n",
    "    for layer_name, activation in activations.items():\n",
    "        print(f\"  {layer_name}: {activation.shape}\")\n",
    "    \n",
    "    # Get layer information\n",
    "    layer_info = extractor.get_layer_info(model)\n",
    "    print(\"\\nLayer Information:\")\n",
    "    for name, info in layer_info.items():\n",
    "        print(f\"  {name}: {info['type']}, Parameters: {info['params']:,}\")\n",
    "else:\n",
    "    print(\"Skipping activation extraction - TensorFlow not available\")\n",
    "    # Create dummy activations for demonstration\n",
    "    activations = {\n",
    "        'layer_1': np.random.randn(len(X_test), 50),\n",
    "        'layer_2': np.random.randn(len(X_test), 40),\n",
    "        'layer_3': np.random.randn(len(X_test), 20),\n",
    "        'output': np.random.randn(len(X_test), 1)\n",
    "    }\n",
    "    print(\"Using dummy activations for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topological Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize topological analyzer\n",
    "analyzer = TopologicalAnalyzer(homology_dimensions=[0, 1, 2])\n",
    "\n",
    "print(\"Performing topological analysis...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Perform comprehensive topological analysis\n",
    "results = analyzer.analyze_network_topology(\n",
    "    activations=activations,\n",
    "    labels=y_test,\n",
    "    compute_mapper=True,\n",
    "    compute_umap=True\n",
    ")\n",
    "\n",
    "print(\"\\nTopological analysis complete!\")\n",
    "print(f\"Analyzed {results.metadata['layer_count']} layers\")\n",
    "print(f\"Sample count: {results.metadata['sample_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Betti numbers (topological features)\n",
    "print(\"Betti Numbers (Topological Features) by Layer:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for layer_name, betti_nums in results.betti_numbers.items():\n",
    "    print(f\"\\n{layer_name.upper()}:\")\n",
    "    print(f\"  H0 (Connected Components): {betti_nums.get(0, 0)}\")\n",
    "    print(f\"  H1 (Loops): {betti_nums.get(1, 0)}\")\n",
    "    print(f\"  H2 (Voids): {betti_nums.get(2, 0)}\")\n",
    "    total_features = sum(betti_nums.values())\n",
    "    print(f\"  Total Features: {total_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Betti number evolution across layers\n",
    "layer_names = list(results.betti_numbers.keys())\n",
    "h0_values = [results.betti_numbers[name].get(0, 0) for name in layer_names]\n",
    "h1_values = [results.betti_numbers[name].get(1, 0) for name in layer_names]\n",
    "h2_values = [results.betti_numbers[name].get(2, 0) for name in layer_names]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "x_pos = range(len(layer_names))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar([x - width for x in x_pos], h0_values, width, label='H0 (Components)', alpha=0.8)\n",
    "plt.bar(x_pos, h1_values, width, label='H1 (Loops)', alpha=0.8)\n",
    "plt.bar([x + width for x in x_pos], h2_values, width, label='H2 (Voids)', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Betti Number')\n",
    "plt.title('Topological Feature Evolution Across Layers')\n",
    "plt.xticks(x_pos, [name.replace('dense_', 'Layer ') for name in layer_names], rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot UMAP projections for each layer\n",
    "n_layers = len(results.umap_projections)\n",
    "if n_layers > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (layer_name, projection) in enumerate(results.umap_projections.items()):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        ax = axes[i]\n",
    "        scatter = ax.scatter(projection[:, 0], projection[:, 1], \n",
    "                           c=y_test, cmap='RdYlBu', alpha=0.7, s=30)\n",
    "        ax.set_title(f'UMAP: {layer_name.replace(\"dense_\", \"Layer \")}')\n",
    "        ax.set_xlabel('UMAP 1')\n",
    "        ax.set_ylabel('UMAP 2')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.colorbar(scatter, ax=ax)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_layers, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Data Geometry Evolution Through Network Layers', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No UMAP projections available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute and Display Topology Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute topology metrics\n",
    "topology_metrics = analyzer.compute_topology_metrics(results)\n",
    "\n",
    "print(\"Topology Metrics:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Group metrics by layer\n",
    "layer_metrics = {}\n",
    "for metric_name, value in topology_metrics.items():\n",
    "    parts = metric_name.split('_')\n",
    "    layer_name = '_'.join(parts[:-2])  # Everything except last 2 parts\n",
    "    metric_type = '_'.join(parts[-2:])  # Last 2 parts\n",
    "    \n",
    "    if layer_name not in layer_metrics:\n",
    "        layer_metrics[layer_name] = {}\n",
    "    layer_metrics[layer_name][metric_type] = value\n",
    "\n",
    "for layer_name, metrics in layer_metrics.items():\n",
    "    print(f\"\\n{layer_name.upper()}:\")\n",
    "    for metric_type, value in metrics.items():\n",
    "        print(f\"  {metric_type.replace('_', ' ').title()}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nDataset: {len(y_test)} samples, {X_test.shape[1]} features\")\n",
    "if tensorflow_available:\n",
    "    print(f\"Model accuracy: {test_acc:.3f}\")\n",
    "print(f\"Layers analyzed: {len(activations)}\")\n",
    "\n",
    "print(\"\\nKey Topological Insights:\")\n",
    "\n",
    "# Find layer with most topological complexity\n",
    "total_features_by_layer = {}\n",
    "for layer_name, betti_nums in results.betti_numbers.items():\n",
    "    total_features_by_layer[layer_name] = sum(betti_nums.values())\n",
    "\n",
    "if total_features_by_layer:\n",
    "    most_complex_layer = max(total_features_by_layer, key=total_features_by_layer.get)\n",
    "    max_complexity = total_features_by_layer[most_complex_layer]\n",
    "    print(f\"â€¢ Most topologically complex layer: {most_complex_layer} ({max_complexity} features)\")\n",
    "\n",
    "# Analyze feature evolution\n",
    "layer_order = list(results.betti_numbers.keys())\n",
    "if len(layer_order) >= 2:\n",
    "    first_layer_features = sum(results.betti_numbers[layer_order[0]].values())\n",
    "    last_layer_features = sum(results.betti_numbers[layer_order[-1]].values())\n",
    "    \n",
    "    if last_layer_features > first_layer_features:\n",
    "        print(\"â€¢ Topological complexity increases through the network\")\n",
    "    elif last_layer_features < first_layer_features:\n",
    "        print(\"â€¢ Topological complexity decreases through the network\")\n",
    "    else:\n",
    "        print(\"â€¢ Topological complexity remains stable through the network\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"â€¢ H0 (connected components): Number of distinct clusters in the data\")\n",
    "print(\"â€¢ H1 (loops): Circular/cyclical patterns in the feature space\")\n",
    "print(\"â€¢ H2 (voids): Higher-dimensional holes or cavities in the data\")\n",
    "print(\"â€¢ UMAP projections show how data geometry evolves through layers\")\n",
    "\n",
    "print(\"\\nAnalysis complete! ðŸŽ‰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}